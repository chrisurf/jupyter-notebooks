{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Email Reply Assistant with LM Studio\n",
                "This notebook demonstrates how to connect to a local Language Model (LM) Studio server and use it to generate email replies."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Step 1: Install required libraries (if not already installed)\n",
                "%pip install requests"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Step 2: Import necessary libraries\n",
                "import requests\n",
                "import json"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Step 3: Define connection details to your local LM Studio server\n",
                "LM_STUDIO_URL = \"http://localhost:1234/v1/chat/completions\""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Step 4: Function to generate content using LM Studio\n",
                "def generate_content(prompt, max_tokens=100, temperature=0.7, model=\"lmstudio-community/Meta-Llama-3-8B-Instruct-GGUF/Meta-Llama-3-8B-Instruct-Q4_K_M.gguf\"):\n",
                "    headers = {\n",
                "        'Content-Type': 'application/json',\n",
                "    }\n",
                "    payload = {\n",
                "        \"model\": model,\n",
                "        \"messages\": [\n",
                "            {\"role\": \"user\", \"content\": prompt}\n",
                "        ],\n",
                "        \"max_tokens\": max_tokens,\n",
                "        \"temperature\": temperature,\n",
                "    }\n",
                "    \n",
                "    try:\n",
                "        response = requests.post(LM_STUDIO_URL, headers=headers, json=payload)\n",
                "        response.raise_for_status()  # Raise an exception for bad status codes\n",
                "        \n",
                "        result = response.json()\n",
                "        return result['choices'][0]['message']['content']\n",
                "    except requests.exceptions.RequestException as e:\n",
                "        print(f\"Error: {e}\")\n",
                "        return \"\""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Step 5: Load email content from a text file\n",
                "FILE_NAME = \"../email.txt\"\n",
                "with open(FILE_NAME, 'r') as file:\n",
                "    incoming_email = file.read()\n",
                "print(\"Incoming Email:\")\n",
                "print(incoming_email)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Step 6: Analyze the incoming email\n",
                "analysis_prompt = f\"Analyze the following email and extract the key points: {incoming_email}\"\n",
                "analysis = generate_content(analysis_prompt)\n",
                "print(\"Analysis:\")\n",
                "print(analysis)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Step 7: Generate a response based on the analysis\n",
                "response_prompt = f\"Generate a polite and professional reply to the following email: {incoming_email}\\nAnalysis: {analysis}\"\n",
                "response = generate_content(response_prompt, max_tokens=200)\n",
                "print(\"Generated Response:\")\n",
                "print(response)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Step 8: Refine and proofread the generated response\n",
                "refine_prompt = f\"Refine and proofread the following email reply: {response}\"\n",
                "refined_response = generate_content(refine_prompt, max_tokens=150)\n",
                "print(\"Refined Response:\")\n",
                "print(refined_response)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
